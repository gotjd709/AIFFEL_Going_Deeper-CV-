{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc01747f",
   "metadata": {},
   "source": [
    "# Project7-Kalman_Filter\n",
    "\n",
    "### 목표\n",
    "동영상이나 실시간으로 스티커 붙이는 실험해보기\n",
    "\n",
    "#### 라이브러리 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a9f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoClip, VideoFileClip\n",
    "from moviepy.editor import ipython_display\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c94e9",
   "metadata": {},
   "source": [
    "#### moviepy를 이용해서 주피터 노트북 상에서 비디오를 읽고 쓰는 프로그램을 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3c5dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/404 [00:00<?, ?it/s, now=None]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/aiffel-dj63/aiffel/video_sticker_app/images/mvpyresult.mp4.\n",
      "MoviePy - Writing audio in mvpyresultTEMP_MPY_wvf_snd.mp3\n",
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/aiffel-dj63/aiffel/video_sticker_app/images/mvpyresult.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/aiffel-dj63/aiffel/video_sticker_app/images/mvpyresult.mp4\n"
     ]
    }
   ],
   "source": [
    "# 읽기\n",
    "video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/video2.mp4'\n",
    "clip = VideoFileClip(video_path)\n",
    "clip = clip.resize(width=640)\n",
    "clip.ipython_display(fps=30, loop=True, autoplay=True, rd_kwargs=dict(logger=None))\n",
    "\n",
    "# 쓰기\n",
    "result_video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/mvpyresult.mp4'\n",
    "clip.write_videofile(result_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152efdb9",
   "metadata": {},
   "source": [
    "#### moviepy 로 읽은 동영상을 numpy 형태로 변환하고 영상 밝기를 50% 어둡게 만든 후에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3411bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  12%|█▏        | 49/403 [00:00<00:00, 485.81it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/aiffel-dj63/aiffel/video_sticker_app/images/mvpyresult2.mp4.\n",
      "Moviepy - Writing video /home/aiffel-dj63/aiffel/video_sticker_app/images/mvpyresult2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/aiffel-dj63/aiffel/video_sticker_app/images/mvpyresult2.mp4\n"
     ]
    }
   ],
   "source": [
    "# 읽기\n",
    "video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/video2.mp4'\n",
    "clip = VideoFileClip(video_path)\n",
    "clip = clip.resize(width=640)\n",
    "clip.ipython_display(fps=30, loop=True, autoplay=True, rd_kwargs=dict(logger=None))\n",
    "\n",
    "# clip 에서 numpy 로 데이터 추출\n",
    "vlen = int(clip.duration*clip.fps)\n",
    "video_container = np.zeros((vlen, clip.size[1], clip.size[0], 3), dtype=np.uint8)\n",
    "for i in range(vlen):\n",
    "    img = clip.get_frame(i/clip.fps)\n",
    "    video_container[i] = (img * 0.5).astype(np.uint8)\n",
    "\n",
    "# 새 clip 만들기\n",
    "dur = vlen / clip.fps\n",
    "outclip = VideoClip(lambda t: video_container[int(round(t*clip.fps))], duration=dur)\n",
    "\n",
    "# 쓰기\n",
    "result_video_path2 = os.getenv('HOME')+'/aiffel/video_sticker_app/images/mvpyresult2.mp4'\n",
    "outclip.write_videofile(result_video_path2, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257709f",
   "metadata": {},
   "source": [
    "#### moviepy로 영상을 읽고 쓰는 시간을 측정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647d4a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  12%|█▏        | 48/403 [00:00<00:00, 448.31it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/aiffel-dj63/aiffel/video_sticker_app/images/mvpyresult.mp4.\n",
      "Moviepy - Writing video /home/aiffel-dj63/aiffel/video_sticker_app/images/mvpyresult.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/aiffel-dj63/aiffel/video_sticker_app/images/mvpyresult.mp4\n",
      "[INFO] moviepy time : 3.08ms\n"
     ]
    }
   ],
   "source": [
    "# CASE 1 : moviepy 사용\n",
    "start = cv2.getTickCount()\n",
    "clip = VideoFileClip(video_path)\n",
    "clip = clip.resize(width=640)\n",
    "\n",
    "vlen = int(clip.duration*clip.fps)\n",
    "video_container = np.zeros((vlen, clip.size[1], clip.size[0], 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(vlen):\n",
    "    img = clip.get_frame(i/clip.fps)\n",
    "    video_container[i] = (img * 0.5).astype(np.uint8)\n",
    "\n",
    "dur = vlen / clip.fps\n",
    "outclip = VideoClip(lambda t: video_container[int(round(t*clip.fps))], duration=dur)\n",
    "\n",
    "mvpy_video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/mvpyresult.mp4'\n",
    "outclip.write_videofile(mvpy_video_path, fps=30)\n",
    "\n",
    "time = (cv2.getTickCount() - start) / cv2.getTickFrequency()\n",
    "print (f'[INFO] moviepy time : {time:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c5716",
   "metadata": {},
   "source": [
    "#### OpenCV를 사용할 때와 차이를 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5e89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cv time : 1.59ms\n"
     ]
    }
   ],
   "source": [
    "# CASE 2 : OpenCV 사용\n",
    "start = cv2.getTickCount()\n",
    "vc = cv2.VideoCapture(video_path)\n",
    "\n",
    "cv_video_path = os.getenv('HOME')+'/aiffel/video_sticker_app/images/cvresult.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vw = cv2.VideoWriter(cv_video_path, fourcc, 30, (640,360))\n",
    "\n",
    "vlen = int(vc.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "for i in range(vlen):\n",
    "    ret, img = vc.read()\n",
    "    if ret == False: break\n",
    "    \n",
    "    img_result = cv2.resize(img, (640, 360)) * 0.5\n",
    "    vw.write(img_result.astype(np.uint8))\n",
    "    \n",
    "time = (cv2.getTickCount() - start) / cv2.getTickFrequency()\n",
    "print (f'[INFO] cv time : {time:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6bd00",
   "metadata": {},
   "source": [
    "4. moviepy 를 이용할 때의 장단점을 분석해 봅시다. 주피터 노트북에 답변을 작성해 코드와 함께 제출해 주세요.\n",
    "\n",
    "# 4. moviepy를 이용할 때의 장단점 분석\n",
    "\n",
    "||`moviepy`|`opencv`|\n",
    "|:---:|:---:|:---:|\n",
    "|장점|진행과정과 경로를 함께 볼 수 있다.|속도가 빠르다|\n",
    "|단점|속도가 느리다|(코드를 추가하면 되겠지만) 진행과정과 경로를 볼 수 없다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef177b",
   "metadata": {},
   "source": [
    "---\n",
    "2. 스티커앱을 실행하고 카메라를 고정하고 서서히 멀어져봅니다. 혹은 아주 가까이 다가가 봅니다. 얼굴을 찾지 못하는 거리를 기록해주세요.\n",
    "일반적으로 약 15ch ~ 1m 30cm 범위 사이에서 얼굴 인식이 가능하다고 합니다. 실제로 측정했을 때 어떠한지 결과를 기록해 주세요.\n",
    "\n",
    "\n",
    "### 2. 스티커앱의 얼굴인식 범위\n",
    "- 최소거리 :: 나의 얼굴이 작아서인지 15cm에서도 괜찮다가 10cm정도로 다가오니 꺼졌다.\n",
    "- 최대거리 :: 나의 얼굴이 작아서인지 1m 20cm정도 멀어지니 왕관을 만들어주지 못했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee7192",
   "metadata": {},
   "source": [
    "---\n",
    "3. 다시 자리로 돌아온 후 고개를 상하좌우로 움직여주세요. yaw, pitch, roll 각도의 개념을 직접 실험해 보고 각각 몇 도까지 정상적으로 스티커앱이 동작하는지 기록해주세요.\n",
    "(참고)\n",
    "\n",
    "yaw : y축 기준 회전 → 높이 축 <br/>\n",
    "picth : x축 기준 회전 → 좌우 축 <br/>\n",
    "roll : z축 기준 회전 → 거리 축 <br/>\n",
    "일반적인 허용 범위는 아래와 같다고 알려져 있습니다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "yaw : -45 ~ 45도 <br/>\n",
    "pitch : -20 ~ 30도 <br/>\n",
    "roll : -45 ~ 45도 <br/>\n",
    "실제 측정해 본 결과는 어떠한지 기록해 주세요.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### 3. 각도 측정 결과\n",
    "\n",
    "- 고개를 좌우로 돌리면 화면을 못봐서 카메라를 내 얼굴을 중심으로 좌측으로 돌려보니 45도 쯤에서 카메라가 꺼졌다. \n",
    "- 내 얼굴을 중심으로 위쪽으로 카메라를 들어보니 30도 쯤에서 스티커가 생겼다. 없어졌다를 반복했다.\n",
    "\n",
    "yaw : -45 ~ 45도 <br/>\n",
    "pitch : -20 ~ 30도 <br/>\n",
    "roll : -45 ~ 45도 <br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "이것은 알려진 것과 동일했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28273570",
   "metadata": {},
   "source": [
    "---\n",
    "4. 만들고 싶은 스티커앱의 스펙(허용 거리, 허용 인원 수, 허용 각도, 안정성)을 정해주세요.\n",
    "(예시)\n",
    "\n",
    "- 거리 : 25cm ~ 1m → 너무 가까우면 스티커의 의미가 없음, 셀카봉을 들었을 때의 유효거리 \n",
    "- 인원 수 : 4명 → 4인 가족 기준\n",
    "- 허용 각도 : pitch : -20 ~ 30도, yaw : -45 ~ 45도, roll : -45 ~ 45도 → 화면을 바라볼 수 있는 각도 \n",
    "- 안정성 : 위 조건을 만족하면서 FPPI (false positive per image) 기준 < 0.003, MR (miss rate) < 1 300장당 1번 에러 = 10초=30\\*10에 1번 에러\n",
    "\n",
    "<br/>\n",
    "\n",
    "기준의 이유를 어떻게 정했는지가 중요합니다. → 서비스 관점, 엔지니어링 관점으로 설명하면 좋습니다.\n",
    "\n",
    "### 4. 만들 스티커앱의 허용스펙\n",
    "\n",
    "- 거리: 25cm ~ 1m\n",
    "- 인원수: 3명\n",
    "- 허용각도: \n",
    "    - yaw : -45 ~ 45도 \n",
    "    - pitch : -20 ~ 30도 \n",
    "    - roll : -45 ~ 45도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05c06f",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "- 이 노드를 하는 주에 환경설정 에러로 인해서 못하고 이제야 하게되었는데 하면서 느낀 점은 너무 흥미롭고 신기하였다. Web 캠을 통해 실시간으로 한다는 것 자체가 흥미로웠는데 핸드폰에 어플을 깔고 어플에서 제공하는 url 을 코드에 넣어주면 핸드폰 카메라가 무선으로 연결되는 점도 무척 신기했다. (물론, 방심했다가 핸드폰 데이터를 다써버렸다.. ㅎㅎ..)\n",
    "\n",
    "- Exploration3번을 진행하며 여러 명을 인식하도록 하였던 것을 떠올리며 적용해보았으나 여러 명의 얼굴이 나오면 다음과 같이\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"webcam_sticker.py\", line 42, in <module>\n",
    "    main()\n",
    "  File \"webcam_sticker.py\", line 35, in main\n",
    "    cv2.imshow('show', img_result)\n",
    "TypeError: Expected Ptr<cv::UMat> for argument 'mat'\n",
    "\n",
    "```\n",
    "에러를 내어 실패하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8088d62",
   "metadata": {},
   "source": [
    "## 루브릭\n",
    "\n",
    "1. 웹캠을 통한 스티커앱을 실행하고 다각도로 문제점을 분석하여 보았다.\n",
    "\n",
    "거리, 인원수, 각도 등 다양한 측면에서의 문제점과 기술적 대안을 정리하였다.\n",
    "\n",
    "인원수에 대한 기술적 대안 코드 (newaddsticker.py)\n",
    "\n",
    "```\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "def img2sticker(img_orig, img_sticker, detector_hog, landmark_predictor):\n",
    "    # preprocess\n",
    "    img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # detector\n",
    "    img_rgb_vga = cv2.resize(img_rgb, (640, 360))\n",
    "    dlib_rects = detector_hog(img_rgb_vga, 0) # <- 이미지 피라미드 수 변경\n",
    "    if len(dlib_rects) < 1:\n",
    "        return img_orig\n",
    "\n",
    "    # landmark\n",
    "    list_landmarks = []\n",
    "    for dlib_rect in dlib_rects:\n",
    "        points = landmark_predictor(img_rgb_vga, dlib_rect)\n",
    "        list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "        list_landmarks.append(list_points)\n",
    "    \n",
    "    x = 0\n",
    "    \n",
    "    # head coord\n",
    "    for dlib_rect, landmark in zip(dlib_rects, list_landmarks):\n",
    "        if x == 0:\n",
    "            x = landmark[30][0] # nose\n",
    "            y = landmark[30][1] - dlib_rect.width()//2\n",
    "            w = dlib_rect.width()\n",
    "            h = dlib_rect.width()\n",
    "            if len(dlib_rects) == 1:\n",
    "                break\n",
    "            else: \n",
    "                H = 1\n",
    "        elif H == 1:\n",
    "            x1 = landmark[30][0] # nose\n",
    "            y1 = landmark[30][1] - dlib_rect.width()//2\n",
    "            w1 = dlib_rect.width()\n",
    "            h1 = dlib_rect.width()\n",
    "            # sticker\n",
    "            img_sticker = cv2.resize(img_sticker, (w,h), interpolation=cv2.INTER_NEAREST)\n",
    "            img_sticker1 = cv2.resize(img_sticker, (w1,h1), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            refined_x = x - w // 2\n",
    "            refined_y = y - h\n",
    "\n",
    "            refined_x1 = x1 - w1 // 2\n",
    "            refined_y1 = y1 - h1\n",
    "            \n",
    "            if refined_y < 0:\n",
    "                img_sticker = img_sticker[-refined_y:]\n",
    "                refined_y = 0\n",
    "\n",
    "            if refined_x < 0:\n",
    "                img_sticker = img_sticker[:, -refined_x:]\n",
    "                refined_x = 0\n",
    "            elif refined_x + img_sticker.shape[1] >= img_orig.shape[1]:\n",
    "                img_sticker = img_sticker[:, :-(img_sticker.shape[1]+refined_x-img_orig.shape[1])]\n",
    "\n",
    "            if refined_y1 < 0:\n",
    "                img_sticker1 = img_sticker1[-refined_y1:]\n",
    "                refined_y1 = 0\n",
    "\n",
    "            if refined_x1 < 0:\n",
    "                img_sticker1 = img_sticker1[:, -refined_x1:]\n",
    "                refined_x1 = 0\n",
    "            elif refined_x1 + img_sticker1.shape[1] >= img_orig.shape[1]:\n",
    "                img_sticker1 = img_sticker1[:, :-(img_sticker1.shape[1]+refined_x1-img_orig.shape[1])]\n",
    "\n",
    "            img_bgr = img_orig.copy()\n",
    "            sticker_area = img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]]\n",
    "\n",
    "            img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]] = \\\n",
    "                cv2.addWeighted(sticker_area, 1.0, img_sticker, 0.7, 0)\n",
    "\n",
    "            img_bgr1 = img_orig.copy()\n",
    "            sticker_area1 = img_bgr1[refined_y1:refined_y1+img_sticker1.shape[0], refined_x1:refined_x1+img_sticker1.shape[1]]\n",
    "\n",
    "            img_bgr1[refined_y1:refined_y1+img_sticker1.shape[0], refined_x1:refined_x1+img_sticker1.shape[1]] = \\\n",
    "                cv2.addWeighted(sticker_area1, 1.0, img_sticker1, 0.7, 0)\n",
    "\n",
    "            return img_bgr, img_bgr1\n",
    "                       \n",
    "        \n",
    "    \n",
    "    # sticker\n",
    "    img_sticker = cv2.resize(img_sticker, (w,h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    refined_x = x - w // 2\n",
    "    refined_y = y - h\n",
    "    \n",
    "    if refined_y < 0:\n",
    "        img_sticker = img_sticker[-refined_y:]\n",
    "        refined_y = 0\n",
    "\n",
    "    if refined_x < 0:\n",
    "        img_sticker = img_sticker[:, -refined_x:]\n",
    "        refined_x = 0\n",
    "    elif refined_x + img_sticker.shape[1] >= img_orig.shape[1]:\n",
    "        img_sticker = img_sticker[:, :-(img_sticker.shape[1]+refined_x-img_orig.shape[1])]\n",
    "\n",
    "    img_bgr = img_orig.copy()\n",
    "    sticker_area = img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]]\n",
    "\n",
    "    img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]] = \\\n",
    "        cv2.addWeighted(sticker_area, 1.0, img_sticker, 0.7, 0)\n",
    "\n",
    "    return img_bgr\n",
    "```\n",
    "\n",
    "\n",
    "2. 스티커앱 초기버전이 가진 예외사항들을 분석하여 수정하였다.\n",
    "\n",
    "프레임 좌표범위 예외처리 관련 오류를 수정하였다.\n",
    "\n",
    "\n",
    "\n",
    "3. 칼만 필터를 웹캠 스티커앱에 적용하여 스티커의 안정성을 높여 보았다.\n",
    "\n",
    "칼만 필터를 적용한 스티커앱을 작성하여 실행해 보고 안정성 여부를 확인하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e5880",
   "metadata": {},
   "source": [
    "# webcam_sticker_kf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9887cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "from kfaddsticker import img2sticker_kf\n",
    "\n",
    "detector_hog = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('./models/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def main():\n",
    "    cv2.namedWindow('show', 0)\n",
    "    cv2.resizeWindow('show', 640, 360)\n",
    "\n",
    "    #vc = cv2.VideoCapture(0)\n",
    "    vc = cv2.VideoCapture('./images/video2.mp4')\n",
    "    img_sticker = cv2.imread('./images/king.png')\n",
    "\n",
    "    vlen = int(vc.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print (vlen) # 웹캠은 video length 가 0 입니다.\n",
    "\n",
    "    # 정해진 길이가 없기 때문에 while 을 주로 사용합니다.\n",
    "    # for i in range(vlen):\n",
    "    while True:\n",
    "        ret, img = vc.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        start = cv2.getTickCount()\n",
    "        img = cv2.flip(img, 1)  # 보통 웹캠은 좌우 반전\n",
    "\n",
    "        # 스티커 메소드를 사용\n",
    "        img_result = img2sticker_kf(img, img_sticker.copy(), detector_hog, landmark_predictor)   \n",
    "\n",
    "        time = (cv2.getTickCount() - start) / cv2.getTickFrequency() * 1000\n",
    "        print ('[INFO] time: %.2fms'%time)\n",
    "        \n",
    "        cv2.imshow('show', img_result)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffef60d",
   "metadata": {},
   "source": [
    "# kfaddsticker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83127f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kpkf import tracker\n",
    "\n",
    "detector_hog = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('./models/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "box_tracker = tracker(num_points=2, sys_err=0.5, measure_err=1000)\n",
    "nose_tracker = tracker(num_points=1, sys_err=1., measure_err=100)\n",
    "\n",
    "flg_nose_tracker = True\n",
    "\n",
    "def draw_mid(img_bgr, list_landmarks, list_bbox):\n",
    "    for bbox in list_bbox:\n",
    "        l = bbox.left()\n",
    "        t = bbox.top()\n",
    "        r = bbox.right()\n",
    "        b = bbox.bottom()\n",
    "        l,t,r,b = [ele*2 for ele in [l,t,r,b]]\n",
    "        img_bgr = cv2.rectangle(img_bgr, (l,t), (r,b), (0,255,0), 2, cv2.LINE_AA)\n",
    "    for lm in list_landmarks:\n",
    "        for pt in lm:\n",
    "            pt = tuple([ele*2 for ele in pt])\n",
    "            img_bgr = cv2.circle(img_bgr, pt, 1, (0,128,255), -1, cv2.LINE_AA)\n",
    "    return img_bgr\n",
    "\n",
    "def img2sticker_kf(img_orig, img_sticker, detector_hog, landmark_predictor):\n",
    "    # preprocess\n",
    "    img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "    # detector\n",
    "    img_rgb_vga = cv2.resize(img_rgb, (640, 360))\n",
    "    dlib_rects = detector_hog(img_rgb_vga, 1)\n",
    "    if len(dlib_rects) < 1:\n",
    "        return img_orig\n",
    "    \n",
    "    # tracker\n",
    "    if len(dlib_rects) == 1:\n",
    "        bbox = dlib_rects[0]\n",
    "        list_input = [(bbox.left(), bbox.top()), (bbox.right(), bbox.bottom())]\n",
    "        np_estimate = np.array(box_tracker.process(list_input))\n",
    "        np_est_points = np_estimate.reshape(2, 3)[:,:2].astype(int)\n",
    "        l,t,r,b = np_est_points.flatten()\n",
    "        # print (l,t,r,b)\n",
    "        if (b-t)*(r-l) > 100:\n",
    "            dlib_rects[0] = dlib.rectangle(left=l,top=t,right=r,bottom=b)\n",
    "\n",
    "    # landmark\n",
    "    list_landmarks = []\n",
    "    for dlib_rect in dlib_rects:\n",
    "        points = landmark_predictor(img_rgb_vga, dlib_rect)\n",
    "        list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "        list_landmarks.append(list_points)\n",
    "    \n",
    "    # head coord\n",
    "    for dlib_rect, landmark in zip(dlib_rects, list_landmarks):\n",
    "        x = landmark[30][0] # nose\n",
    "        y = landmark[30][1] \n",
    "        w = dlib_rect.width()\n",
    "        h = dlib_rect.width()\n",
    "        x,y,w,h = [ele*2 for ele in [x,y,w,h]]\n",
    "        break\n",
    "    # sticker\n",
    "    img_sticker = cv2.resize(img_sticker, (w,h), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    if flg_nose_tracker == True:\n",
    "        list_input = [(x, y)]\n",
    "        np_estimate = np.array(nose_tracker.process(list_input))\n",
    "        np_est_points = np_estimate.reshape(1, 3)[:,:2].astype(int)\n",
    "        x_tmp, y_tmp = np_est_points.flatten()\n",
    "        if x_tmp*y_tmp != 0:\n",
    "            x = x_tmp\n",
    "            y = y_tmp\n",
    "\n",
    "    refined_x = x - w // 2\n",
    "    refined_y = y - h - dlib_rect.width()\n",
    "    # print ('(x,y) : (%d,%d)'%(refined_x, refined_y))\n",
    "\n",
    "    if refined_y < 0:\n",
    "        img_sticker = img_sticker[-refined_y:]\n",
    "        refined_y = 0\n",
    "\n",
    "    if refined_x < 0:\n",
    "        img_sticker = img_sticker[:, -refined_x:]\n",
    "        refined_x = 0\n",
    "    elif refined_x + img_sticker.shape[1] >= img_orig.shape[1]:\n",
    "        img_sticker = img_sticker[:, :-(img_sticker.shape[1]+refined_x-img_orig.shape[1])]\n",
    "\n",
    "\n",
    "    img_bgr = img_orig.copy()\n",
    "    sticker_area = img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]]\n",
    "\n",
    "    img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]] = \\\n",
    "        cv2.addWeighted(sticker_area, 1.0, img_sticker, 0.7, 0)\n",
    "\n",
    "    return img_bgr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd215c",
   "metadata": {},
   "source": [
    "# kpkf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12473cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from kalman import KF2d\n",
    "\n",
    "class tracker:\n",
    "    def __init__(self, num_points=17, sys_err=0.95, measure_err=100):\n",
    "        self.l_KF = []\n",
    "        self.l_P  = []\n",
    "        self.l_x  = []\n",
    "        self.num_points = num_points\n",
    "        for n in range(num_points):\n",
    "            KF = KF2d(dt=1)\n",
    "            P = 100 * np.eye(4, dtype=np.float)\n",
    "            x = np.array([0,0,0,0], dtype=np.float)\n",
    "            \n",
    "            self.l_KF.append(KF)\n",
    "            self.l_P.append(P)\n",
    "            self.l_x.append(x)\n",
    "        \n",
    "        self.l_estimate = []\n",
    "        self.keypoints = []\n",
    "\n",
    "    def main_process(self, list_measured_points):\n",
    "        self.l_measure = list_measured_points\n",
    "        for i in range(self.num_points):\n",
    "            point = list_measured_points[i]\n",
    "            z = np.array(point, dtype=np.float)\n",
    "\n",
    "            self.l_x[i], self.l_P[i], filtered_point = self.l_KF[i].process(self.l_x[i], self.l_P[i], z)\n",
    "            \n",
    "            self.l_estimate.append(filtered_point)\n",
    "\n",
    "    def preprocess(self, list_measured_points):\n",
    "        return list_measured_points\n",
    "    \n",
    "    def postprocess(self):\n",
    "        cnt_validpoint = 0\n",
    "        x_vel_sum, y_vel_sum = 0, 0\n",
    "        for i in range(self.num_points):\n",
    "            if self.l_x[i][0] > 10 and self.l_x[i][2] > 10:\n",
    "                x_vel_sum += abs(self.l_x[i][1])\n",
    "                y_vel_sum += abs(self.l_x[i][3])\n",
    "                cnt_validpoint += 1\n",
    "\n",
    "        x_vel_mean = x_vel_sum / cnt_validpoint if cnt_validpoint != 0 else 0\n",
    "        y_vel_mean = y_vel_sum / cnt_validpoint if cnt_validpoint != 0 else 0\n",
    "        \n",
    "        for i in range(self.num_points):\n",
    "            if x_vel_mean > 10.0 or y_vel_mean > 10.0:\n",
    "                self.l_estimate[i] = self.l_measure[i]\n",
    "                x, y = self.l_measure[i]\n",
    "                self.l_x[i] = np.array([x,0,y,0], dtype=np.float)\n",
    "\n",
    "            v = 2 if self.l_estimate[i][0] > 10 and self.l_estimate[i][1] > 10 else 0\n",
    "            self.keypoints += list(self.l_estimate[i]) + [v]\n",
    "        \n",
    "    def process(self, list_measured_points):\n",
    "        self.keypoints = []\n",
    "        self.l_estimate = []\n",
    "        \n",
    "        # self.preprocess(list_measured_points)\n",
    "        self.main_process(list_measured_points)\n",
    "        self.postprocess()\n",
    "        \n",
    "        return self.keypoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521378df",
   "metadata": {},
   "source": [
    "# kalman.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39634065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class KF2d():\n",
    "    '''\n",
    "        x = [x_position, x_velocity, y_position, y_velocity]\n",
    "    '''\n",
    "    def __init__(self, dt=1):\n",
    "        super(KF2d, self).__init__()\n",
    "        self.dt = dt\n",
    "        self.A = np.array([\n",
    "            [1, dt, 0,  0],\n",
    "            [0,  1, 0,  0],\n",
    "            [0,  0, 1, dt],\n",
    "            [0,  0, 0,  1],\n",
    "        ], dtype=np.float)\n",
    "        self.H = np.array([\n",
    "            [1, 0, 0, 0],\n",
    "            [0, 0, 1, 0]\n",
    "        ])\n",
    "        self.Q = 0.9*np.eye(4, dtype=np.float)\n",
    "        self.R = np.array([\n",
    "            [100, 0],\n",
    "            [0, 100]\n",
    "        ], dtype=np.float)\n",
    "        \n",
    "        self.zero_cnt = 0\n",
    "        self.flg_disappear = True\n",
    "    \n",
    "    def kalman_main(self, x, P, z):\n",
    "        ''' prediction '''\n",
    "        xp = self.A @ x\n",
    "        Pp = self.A @ P @ self.A.T + self.Q\n",
    "        \n",
    "        ''' kalman gain '''\n",
    "        # ''' original code '''\n",
    "        # K = Pp @ self.H.T @ np.linalg.inv( self.H @ Pp @ self.H.T + self.R)\n",
    "\n",
    "        # ''' optimization code : remove matrix inverse '''\n",
    "        # 40% fatser than original code\n",
    "        HPH = np.array([\n",
    "            [Pp[0,0], Pp[0,2]],\n",
    "            [Pp[2,0], Pp[2,2]],\n",
    "        ])\n",
    "        HPHR = HPH + self.R\n",
    "        inv_HPHR = np.array([\n",
    "                [HPHR[1,1], -HPHR[0,1]],\n",
    "                [-HPHR[1,0], HPHR[0,0]]\n",
    "                ])\n",
    "        inv_HPHR /= (HPHR[0,0]*HPHR[1,1]-HPHR[0,1]*HPHR[1,0])\n",
    "        K = Pp @ self.H.T @ inv_HPHR\n",
    "\n",
    "        ''' estimation '''\n",
    "        x = xp + K @ (z - self.H @ xp)\n",
    "        P = Pp - K @ self.H @ Pp        # Error covariance matrix\n",
    "        \n",
    "        return x, P\n",
    "\n",
    "    def preprocess(self, x, P, z):\n",
    "        '''\n",
    "        Not measured case\n",
    "        zero count++ , replacec z into estimate\n",
    "        '''\n",
    "        if z[0]!=0 and z[1]!=0 and self.flg_disappear == True:\n",
    "            self.flg_disappear = False\n",
    "            x[0] = z[0]\n",
    "            x[1] *= 0.1\n",
    "            x[2] = z[1]\n",
    "            x[3] *= 0.1\n",
    "            P = 0 * np.eye(4, dtype=np.float)\n",
    "\n",
    "        if x[0]!=0 and x[2]!=0 and z[0]==0 and z[1]==0:\n",
    "            z[0] = x[0]\n",
    "            z[1] = x[2]\n",
    "            self.zero_cnt += 1\n",
    "        else:\n",
    "            self.zero_cnt = 0\n",
    "\n",
    "        if self.zero_cnt >= 5:\n",
    "            self.zero_cnt = 0\n",
    "            self.flg_disappear = True\n",
    "            x = np.array([0,0,0,0], dtype=np.float)\n",
    "            P = 0 * np.eye(4, dtype=np.float)\n",
    "\n",
    "        if abs(x[1]) > 5 or abs(x[3]) > 5:\n",
    "            x[0] = z[0]\n",
    "            x[1] *= 0.1\n",
    "            x[2] = z[1]\n",
    "            x[3] *= 0.1\n",
    "            P = 0 * np.eye(4, dtype=np.float)\n",
    "        \n",
    "        return x, P, z\n",
    "\n",
    "    def postprocess(self, x, P):\n",
    "\n",
    "        if abs(x[1]) > 10 or abs(x[3]) > 10:\n",
    "            output = (0,0)\n",
    "        else:\n",
    "            output = (int(round(x[0])), int(round(x[2])))\n",
    "        if x[0] < 10 or x[2] < 10:\n",
    "            output = (0,0)\n",
    "\n",
    "        return x, P, output\n",
    "\n",
    "    def process(self, x, P, z):\n",
    "        o = None\n",
    "        \n",
    "        x, P, z = self.preprocess(x, P, z)\n",
    "        x, P    = self.kalman_main(x, P, z)\n",
    "        x, P, o = self.postprocess(x, P)\n",
    "\n",
    "        return x, P, o\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
